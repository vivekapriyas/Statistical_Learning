---
subtitle: "TMA4268 Statistical Learning V2021"
title: "Compulsory exercise 2: Group 27 "
author: "Maren Bråthen Kristoffersen, Vilde Marie Skårdal Jensen and Viveka Priya Simhan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```



```{r, echo=FALSE, eval=TRUE}
library("knitr") #probably already installed
library("rmarkdown") #probably already installed
library("ggplot2") #plotting with ggplot
library("ggfortify")
library("leaps")
library("glmnet")
library("tree")
library("caret")
library("randomForest")
library("readr")
library("e1071")
library("dplyr")
library("gbm")
```

```{r test, include=FALSE}
options(tinytex.verbose = TRUE)
warnings()
```

### Problem 1

### a) Multiple choice 1
(i) TRUE

Påstand: Regualarizaton will reduce test error, but not training error:
-  By constraining or shrinking the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias. This can lead to substantial improvements in the accuracy with which we can predict the response for observations not used in model training.
- It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance.

(ii) TRUE
(iii) TRUE

(iv) FALSE Påstand: PCR can be used for variable selection
-  However, as a result of the way PCR is implemented, the final model is more difficult to interpret because it does not perform any kind of variable selection or even directly produce coefficient estimates. (page 257)
- We note that even though PCR provides a simple way to perform regression using M<p predictors, it is not a feature selection method. This is because each of the M principal components used in the regression is a linear combination of all p of the original features.


```{r, echo=FALSE, eval=TRUE}
id <- "1iI6YaqgG0QJW5onZ_GTBsCvpKPExF30G"
catdat <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
```
```{r, echo=FALSE, eval=TRUE}
set.seed(4268)
train.ind = sample(1:nrow(catdat), 0.5 * nrow(catdat))
catdat.train = catdat[train.ind, ]
catdat.test = catdat[-train.ind, ]
```


### b) Best subset selection
For this task, we will perform best subset selection to obtain a reduced linear regression model based on the ``catdat`` dataset. Below, we have used the function ``regsubsets()``to implement the best subset selection method.
```{r, echo=TRUE, eval=TRUE}
numPred = 17
BSS = regsubsets(birds~., data = catdat.train, nvmax = numPred)
BSS_summary = summary(BSS)
```

By observing the graphs below, that is illustrating the optimal numbers of predictors for different criterions, we want to determine which model choice criterion to use. 
```{r, echo=FALSE, eval=TRUE}
#plot illustrating different model choice criterions
plot(BSS_summary$rss, xlab = "Numbers of variables", ylab = "RSS", type ="l") 

plot(BSS_summary$bic, xlab = "Numbers of variables", ylab = "BIC", type ="l")
BSS_best_bic <- which.min(BSS_summary$bic)
points(BSS_best_bic, BSS_summary$bic[BSS_best_bic], col = "red", cex = 2, pch = 20)

plot(BSS_summary$adjr2, xlab = "Numbers of variables", ylab = "R_{adj}^2", type ="l")
BSS_best_adjr2 = which.max(BSS_summary$adjr2)
points(BSS_best_adjr2, BSS_summary$adjr2[BSS_best_adjr2], col = "red", cex = 2, pch = 20)

plot(BSS_summary$cp,xlab = "Numbers of variables", ylab = "Cp", type ="l")
BSS_best_cp = which.min(BSS_summary$cp)
points(BSS_best_cp, BSS_summary$cp[BSS_best_cp], col = "red", cex = 2, pch = 20)
```

The first plot shows that the RSS increases for increasing number of predictors, which is expected since RSS does not punish the model for having many predictors. When wanting to minimize the Bayesian Information Criterion (BIC), one sees that the subset containing six predictors turns out as the best subset. For the adjusted R-squared and $C_p$, one can see that there are no specific subset that stands out to be the best, and it looks like all subsets containing between 5 and 17 predictors results in approximately the same model performance. However, all the plots indicates that the reduced model should include at least the five predictors ``r names(coef(BSS, 5))``. Kept in mind that we want our model to be as simple as possible, we have chosen BIC as model choice criterion. 


The above plot illustrates the best subsets ranked based on BIC, where the best subset in at the top.

``` {r, echo=FALSE, eval=TRUE}
#plot illustrating which parameters that is used in the prefered model
plot(BSS, scale = "bic")
coef(BSS, BSS_best_bic)
```
The above plot illustrates the best subsets ranked based on BIC, where the best subset in at the top. Thus, the selected variables is  ``wetfood``, ``daily.playtime``, ``children.13``, ``urban``, ``bell`` and ``daily.outdoortime``.


To determine the mean square error we use this reduced linear regression model to predict the number of birds eaten by cats. 
``` {r, echo=TRUE, eval=TRUE}
#linear regression using model criterion BIC
kept_predictors <- names(coef(BSS, id = BSS_best_bic))
kept_predictors <- kept_predictors[!kept_predictors %in% "(Intercept)"]
BSS_design_matrix <- model.matrix(birds~., catdat.train)[, kept_predictors]
BSS_data_train <- data.frame(birds = catdat.train$birds, BSS_design_matrix)

LinReg_BIC <- lm(birds~., BSS_data_train)

#make predictions on the test set
BSS_design_matrix_test <- model.matrix(birds~., catdat.test)[, kept_predictors]
BSS_predictions <- predict(LinReg_BIC, newdata = as.data.frame(BSS_design_matrix_test))

#compute test squared errors
BSS_squared_errors <- (catdat.test$birds - BSS_predictions)^2
squared_errors <- data.frame(BSS_squared_errors = BSS_squared_errors)

#the test MSE
MSE_BSS <- mean(BSS_squared_errors)
MSE_BSS
```
The test mean square error for the reduced model using best subset selection with 6 predictors is ``r MSE_BSS``. 

### c) Lasso Regression
Now, we are going to obtain a lasso regression model of the dataset ``catdat``.  
```{r}
#make matrices of the training and testing data
x.train <- model.matrix(birds~., data = catdat.train)[, -1]
y.train <- catdat.train$birds
x.test <- model.matrix(birds~., data = catdat.test)[,-1]
y.test <- catdat.test$birds

#fits a genelarized linar model with lasso penalty.
lasso_mod <- glmnet(x.train, y.train, alpha = 1)

#k-fold cross validation to choose lambda
set.seed(1)
cv.out = cv.glmnet(x.train, y.train, alpha = 1)
plot(cv.out)

bestlam.Lasso<- cv.out$lambda.min

lasso_predictions = predict(lasso_mod, s = bestlam.Lasso, newx= x.test)
lasso_square_errors <- as.numeric((lasso_predictions - y.test)^2)
squared_errors_lasso<- data.frame(lasso_square_errors = lasso_square_errors, squared_errors)

#determine the coefficients used in the lasso regression
lasso.coef = predict(lasso_mod, type = "coefficients", s = bestlam.Lasso)[1:18,]
all_coefs = lasso.coef
nonzero_coefs = all_coefs[all_coefs!=0]

#the test MSE
MSE_lasso <- mean(lasso_square_errors)
MSE_lasso
```
For this method, $\lambda$ is chosen using cross-validation, where the $\lambda$ that minimizes the cross-validated error is selected and was found to be $\lambda = $ ``r bestlam.Lasso ``. 

MÅ SI NOE MER OM DENNE KROSSVALIDERINGA. 


A beneficial property of the lasso regression is that predictors can be shrinked to all the way to zero, meaning that parameters that is not good to estimate for the response can be completely erased from the model. In our case, lasso regression uses the predictors ``r names(lasso.coef[lasso.coef!=0])`` and get rid of the predictors ``r names(lasso.coef[lasso.coef == 0])``.

The test mean square error for the lasso regression model is ``r MSE_lasso``. 



### d) 

For increased values of $\lambda$ the slopes gets smaller and smaller until they reach zero as $\lambda \to \infty$. This means that one is left with a model containing only the intercept. On the other hand, when $\lambda =0$ the lasso regression line will be the same as for the least squared regression line, i.e. the full multiple linear regression, since the term that provides for penalization vanishes. 

### e)
We will now show that the test MSE of the best subset selection model and the lasso regression model indeed are better  than the test MSE for the model with only intercept and the full model. We find these test MSE's the following way
```{r, echo=TRUE, eval=TRUE }
#determine MSE for a model containing only the intercept
ILR <- lm(birds~1, data = catdat.train)

ILR_squared_errors <- (catdat.test$birds - summary(ILR)$coefficients[1])^2

MSE_ILR <- mean(ILR_squared_errors)


#determine MSE for a multiple linear regression
MLR <- lm(birds~., data = catdat.train)

MLR_predictions <- predict(MLR, newdata = catdat.test)
MLR_squared_errors <- (catdat.test$birds - MLR_predictions)^2

MSE_MLR <- mean(MLR_squared_errors)
```
The test mean square errors for respectively the model containing only intercept and the multiple linear regression model were found to be ``r MSE_ILR`` and ``r MSE_MLR``. Both of these values are greater than the once obtained for best subset selection with BIC as model criteria and lasso regression. Hence, the models from b) and c) results in better predictions than both the full model and the model with no predictors. 

Kanskje dra en sammenheng til at penalty ved lasso regression er hensiktsmessig. 


### f)
```{r, echo=FALSE, eval = TRUE}
mse_values <- matrix(c(MSE_BSS, MSE_lasso, MSE_MLR, MSE_ILR), ncol = 1, byrow = TRUE)
colnames(mse_values) <- c("the test MSE")
rownames(mse_values) <- c("Best Subset Selection", "Lasso Regression", "Multiple Linear Regression", "Intercept Linear Regression")
mse_values <- as.table(mse_values)
mse_values
```

From this tabulate, one sees that mean square errors for 

- It turns out that the model with only intercept performs a much worse than the others. This is due to that.. 



### Problem 2

### a)
(i) TRUE
(ii) TRUE
(iii) FALSE
(iv) FALSE

### b)


### c)
```{r,fig.height=6,fig.width=12}

pr.train <- catdat.train[c("birds", "daily.outdoortime")]
deg = 1:10

plot(pr.train, col ='darkgrey', main='Polynomial regression')
co = rainbow(length(deg))


MSE.pr <- rep('numeric',lenght=length(deg))
for (d in deg) {
  mod <- lm(birds ~ poly(daily.outdoortime,d),pr.train)
  
  lines(cbind(pr.train$daily.outdoortime,mod$fit)[order(pr.train$daily.outdoortime),],col=co[d])

  pred <- predict(mod,pr.train)
  MSE.pr[d] <- mean((pred-pr.train$birds)^2)
  
}
legend("topright",legend = paste("d=",deg),lty=1,col=co)

plot(deg,MSE.pr,type="o",xlab="degree",ylab="MSE",main="Training MSE")
```


### Problem 3

### a)
(i) TRUE
(ii) TRUE
(iii) FALSE
(iv) FALSE

### b)
The three will have a split at age < 81.5 and a spilt at country: Indonesia, japan, korea. (Her er jeg litt usikker på hvordan man skal forklare treet). This is the pruned three since the length of the branches says something about how much the RSS decrease when one goes alongside that branch. When we make a regression three we want the RSS to be as small as possible. Hence, we want to keep the branches which are the longest. (Veldig dårlig forklarning, men er sånn ish dette vi vil si tror jeg.)

### c) 

```{r}
id <- "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E" # google file ID
d.diabetes <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
d.train=d.diabetes$ctrain
d.test=d.diabetes$ctest

d.train$diabetes = factor(d.train$diabetes, levels=c('0','1'),labels=c('0','1'))
d.test$diabetes = factor(d.test$diabetes, levels=c('0','1'),labels=c('0','1'))

response.test = d.test$diabetes
```

(i)
```{r}
tree.diabetes = tree(diabetes ~., data = d.train)

plot(tree.diabetes)
text(tree.diabetes, pretty = 0)

yhat.tree = predict(tree.diabetes, newdata = d.test, type = "class")
misclass.tree = table(yhat.tree, response.test)
misclass.tree

1 - sum(diag(misclass.tree))/sum(misclass.tree)

set.seed(1)
cv.diabetes = cv.tree(tree.diabetes, K = 10)
tree.min = which.min(cv.diabetes$dev)
best = cv.diabetes$size[tree.min]
best
plot(cv.diabetes$size, cv.diabetes$dev, type = "b")
points(best, cv.diabetes$dev[tree.min], col = "red", pch = 20)


prune.diabetes = prune.misclass(tree.diabetes, best = best)
plot(prune.diabetes)
text(prune.diabetes, pretty = 0)

yhat.prune = predict(prune.diabetes, newdata = d.test, type = "class")

misclass.prune = table(yhat.prune, response.test)
misclass.prune

1 - sum(diag(misclass.prune))/sum(misclass.prune)

```

```{r}
rf.diabetes = randomForest(diabetes~., data = d.train, mtry = 3, ntree = 500, importance = TRUE)
yhat.rf = predict(rf.diabetes, newdata = d.test)
misclass.rf = table(yhat.rf, response.test)
misclass.rf
1 - sum(diag(misclass.rf))/sum(misclass.rf)
importance(rf.diabetes)
varImpPlot(rf.diabetes)
```

Since glu and bmi has the highest MeanDecreaseAccuracy and MeanDecreaseGini are they the most influential predictors of diabetes. This corresponds well to the three we made in 3c) (i) where the two first splits are based on glu and bmi. 


# Problem 4

### a)
(i) TRUE
(ii) TRUE
(iii) FALSE
(iv) TRUE

### b)
```{r, echo=FALSE, eval=TRUE}
id <- "1x_E8xnmz9CMHh_tMwIsWP94czPa1Fpsj" # google file ID
d.leukemia <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
```
```{r, echo=FALSE, eval=TRUE}
set.seed(2399)
t.samples <- sample(1:60,15,replace=F)
d.leukemia$Category <- as.factor(d.leukemia$Category) 
d.leukemia.test <- d.leukemia[t.samples,]
d.leukemia.train <- d.leukemia[-t.samples,]
```


##### (i)
A SVM is a more suitable approach than logistic regression for this dataset since we have p>>1 and therefore always can find a separating hyperplane, excepting exact feature ties.

One could use hinge loss/logistic regression loss with a penalty term instead of a SVM. 

##### (ii)
The paper suggests using ensemble learning with SVMs to do feature selection on genomic data through elimation. That is, using bootstrapping multiple SVM models are built and then aggregated to form one final model which is used to eliminate genomic features unimportant for patient classification.

##### (iii)

```{r}
svm.leukemia.linear <- svm(Category~., data = d.leukemia.train, kernel = "linear", cost = 1, scale = FALSE)


y.leukemia.train <- predict(svm.leukemia.linear, newdata = d.leukemia.train)
table.leukemia.train <- table(y.leukemia.train, d.leukemia.train$Category)
table.leukemia.train
1 - sum(diag(table.leukemia.train))/sum(table.leukemia.train)

y.leukemia.test <- predict(svm.leukemia.linear, newdata = d.leukemia.test)
table.leukemia.test <- table(y.leukemia.test, d.leukemia.test$Category)
table.leukemia.test
1 - sum(diag(table.leukemia.test))/sum(table.leukemia.test)
```
Training error rate: 0, test error rate: $\frac{3+3}{6+3+3+3} = \frac{6}{15} = 0.4$.
Not surprised over the training error rate since the numer of covariates are much bigger then the number of observations. 
Most common type of error ... 

##### (iv)

```{r}
svm.leukemia.radial1 <- svm(Category~., data = d.leukemia.train, kernel = "radial", cost = 1, gamma = 10^(-2), scale = FALSE)


y.leukemia.train1 <- predict(svm.leukemia.radial1, newdata = d.leukemia.train)
table.leukemia.train1 <- table(y.leukemia.train1, d.leukemia.train$Category)
table.leukemia.train1
1 - sum(diag(table.leukemia.train1))/sum(table.leukemia.train1)

y.leukemia.test1 <- predict(svm.leukemia.radial1, newdata = d.leukemia.test)
table.leukemia.test1 <- table(y.leukemia.test1, d.leukemia.test$Category)
table.leukemia.test1
1 - sum(diag(table.leukemia.test1))/sum(table.leukemia.test1)



svm.leukemia.radial2 <- svm(Category~., data = d.leukemia.train, kernel = "radial", cost = 1, gamma = 10^(-5), scale = FALSE)

y.leukemia.train2 <- predict(svm.leukemia.radial2, newdata = d.leukemia.train)
table.leukemia.train2 <- table(y.leukemia.train2, d.leukemia.train$Category)
table.leukemia.train2
1 - sum(diag(table.leukemia.train2))/sum(table.leukemia.train2)

y.leukemia.test2 <- predict(svm.leukemia.radial2, newdata = d.leukemia.test)
table.leukemia.test2 <- table(y.leukemia.test2, d.leukemia.test$Category)
table.leukemia.test2
1 - sum(diag(table.leukemia.test2))/sum(table.leukemia.test2)



```

### c)

The polynomial kernel for the feature space with inputs $X_1$ and $X_2$ and degree $d=2$ is 
\begin{align*}
  K(\boldsymbol{x}_i, \boldsymbol{x}_{i'}) &=(1+\sum_{j=1}^2x_{ij}x_{i'j})^2 = (1 + x_{i1}x_{i'1} + x_{i2}x_{i'2})^2 \\
  &= 1 + 2x_{i1}x_{i'1} + 2x_{i2}x_{i'2} + (x_{i1}x_{i'1})^2 + 2x_{i1}x_{i'1}x_{i2}x_{i'2} + (x_{i2}x_{i'2})^2.
\end{align*}
This can be written as 
\begin{align*}
    K(\boldsymbol{x}_i, \boldsymbol{x}_{i'}) &= h_1(x)h_1(x') + h_2(x)h_2(x') + h_3(x)h_3(x') + h_4(x)h_4(x') + h_5(x)h_5(x') + h_6(x)h_6(x') \\
    &= \sum_{j=1}^6 h_j(x)h_j(x') = \langle h(x),h(x') \rangle,
\end{align*}

where 
\begin{align*}
    h_1(x)h_1(x') &= 1 \implies h_1(x) = 1 \\
    h_2(x)h_2(x') &= 2x_{i1}x_{i'1} \implies h_2(x) = \sqrt{2}x_{i1}\\
    h_3(x)h_3(x') &= 2x_{i2}x_{i'2}  \implies h_3(x) = \sqrt{2}x_{i2}\\
    h_4(x)h_4(x') &= x_{i1}^2x_{i'1}^2 \implies h_4(x) = x_{i1}^2\\
    h_5(x)h_5(x') &= 2x_{i1}x_{i2}x_{i'1}x_{i'2} \implies h_5(x) = \sqrt{2}x_{i1}x_{i2}\\
    h_6(x)h_6(x') &= x_{i2}^2x_{i'2}^2 \implies h_6(x) = x_{i2}^2.
\end{align*}

# Problem 5
 
### a)
(i) TRUE (page 376)
(ii) FALSE
(iii) FALSE
(iv) FALSE 


### b)
##### (i)
```{r, echo=FALSE, eval=TRUE}
set.seed(1)
x1 = c(1, 2, 0, 4, 5, 6)
x2 = c(5, 4, 3, 1, 1, 2)

n = 6
K = 2



clusters = sample(1:K, n, replace = TRUE)
df = data.frame(x1,x2,clusters = as.factor(clusters))
ggplot(df, aes(x1,x2, color=clusters)) +geom_point()
```
##### (ii)
```{r, echo=FALSE, eval=TRUE}
centroid.cluster1_x1 = mean(df$x1[df$clusters == 1])
centroid.cluster1_x2 = mean(df$x2[df$clusters == 1])
centroid.cluster2_x1 = mean(df$x1[df$clusters == 2])
centroid.cluster2_x2 = mean(df$x2[df$clusters == 2])

df.centroid = data.frame(x1 = c(centroid.cluster1_x1, centroid.cluster2_x1), x2 =c(centroid.cluster1_x2, centroid.cluster2_x2), clusters = as.factor(c(1,2)))

ggplot() + geom_point(data = df, aes(x1,x2, color=clusters)) + geom_point(data = df.centroid, aes(x1,x2, fill = clusters), size = 5, shape = 21, color = "black")
```
##### (iii)
```{r, echo=FALSE, eval=TRUE}
for (i in 1:n){
  euclid.cluster1 = (df$x1[i] - df.centroid$x1[1])^2 + (df$x2[i] - df.centroid$x2[1])^2
  euclid.cluster2 = (df$x1[i] - df.centroid$x1[2])^2 + (df$x2[i] - df.centroid$x2[2])^2
  if (euclid.cluster1 < euclid.cluster2){
    df$clusters[i] = 1
  }
  else{
    df$clusters[i] = 2
  }
}

ggplot() + geom_point(data = df, aes(x1,x2, color=clusters))
```

### c)
```{r, echo=FALSE, eval=TRUE}
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU" # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "")
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "")
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
GeneData = t(GeneData)
GeneData <- scale(GeneData)
```



```{r}
#hierarchical clustering Euclidean distance
par(mfrow=c(2,3))
CE_hclust = hclust(dist(GeneData), method = "complete")
plot(CE_hclust, main = "Complete Linkage, Euclidean", xlab = "", sub = "", cex=.9)

SE_hclust = hclust(dist(GeneData), method = "single")
plot(SE_hclust, main = "Single Linkage, Euclidean", xlab = "", sub = "", cex=.9)

AE_hclust = hclust(dist(GeneData), method = "average")
plot(AE_hclust, main = "Average Linkage, Euclidean", xlab = "", sub = "", cex=.9, xlim = c(40,50))


#hierarchical clustering using correlation-based distance
correlation_dist = as.dist(1-cor(t(GeneData)))
CC_hclust = hclust(correlation_dist, method = "complete")
plot(CC_hclust, main = "Complete Linkage, Correlation-based distance", xlab = "", sub = "", cex=.9)

SC_hclust = hclust(correlation_dist, method = "single")
plot(SC_hclust, main = "Single Linkage, Correlation-based distance", xlab = "", sub = "", cex=.9)

AC_hclust = hclust(correlation_dist, method = "average")
plot(AC_hclust, main = "Average Linkage, Correlation-based distance", xlab = "", sub = "", cex=.9)
```


#### d) 
```{r}
#cut the clusters in two groups
CE_2groups =  cutree(CE_hclust, 2)

SE_2groups =  cutree(SE_hclust, 2)

AE_2groups =  cutree(AE_hclust, 2)

CC_2groups = cutree(CC_hclust, 2)

SC_2groups = cutree(SC_hclust, 2)

AC_2groups = cutree(AC_hclust, 2)

```




### e)
##### (i)
```{r, echo=FALSE, eval=TRUE}
pca.GeneData <- prcomp(GeneData,scale = TRUE) 

#i)
autoplot(pca.GeneData, data = GeneData)
```

##### (ii)
```{r, echo =FALSE,eval=TRUE}
pca.var <- pca.GeneData$sdev^2
pve <- pca.var/sum(pca.var)
explained_var <- sum(pve[1:5])
```
The first 5 PCs explain `r round(explained_var*100,2)`% of the variance. 
